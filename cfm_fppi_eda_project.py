# -*- coding: utf-8 -*-
"""CFM_FPPI EDA PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAapSlfgZPZLKg2vkz7Ylj8dpnHIiZzI

### **INITIALISATION PROCESS**

# Installation of relevant libraries
"""

!pip install sweetviz

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

"""# Uploading of EGIB and EGTB counterparties records with outstanding as at 31 December 2022. 

We encountered error during upload where there is a need to add specific encoding since there are records within  the same field where the data type varies. The conversion from object to string also needed to be specified individually since we encountered error using codes meant to cover the entire fields. We chose not to use "Low Memory = null" method since this code is literally to ignore errors which in our opinion is not the correct way. 
"""

file = "BUDEC2022.csv"
BUD = pd.read_csv(file, encoding = "latin1", dtype={'2 MTHS': 'str', '3 MTHS': 'str','7 MTHS': 'str','8 MTHS': 'str','10 MTHS': 'str','SST ': 'str','0-14 DAYS': 'str','15-30 DAYS': 'str','1 MTH': 'str','4 MTHS': 'str', '5 MTHS': 'str', '6 MTHS': 'str','9 MTHS': 'str','11 MTHS': 'str','12 MTHS': 'str','CLIENT NO': 'str','CLIENT NO.2': 'str','AGNTNUM': 'str','>12 MTHS': 'str','TOTAL': 'str','CLT.1': 'str'})

"""Checking the number of records in December 2022 and the number of fields. """

BUD.shape

"""We decided to replace space with Underscore to minimise errors in subsequent codes"""

BUD.columns = BUD.columns.str.replace(' ', '_')
BUD.info()

"""## **SWEETVIZ**

# Our first Sweetviz Attempt

We decided to run Sweetviz using our Dec 2022 records in order to have better understanding on what kind of graphs that will be generated by Sweetviz.
"""

import sweetviz as sv

analyze_report = sv.analyze(BUD)
analyze_report.show_html('analyze.html', open_browser=False)

"""TADA! Below are the graphs where we found some to be irrelevant to the data we had chosen. """

import IPython
IPython.display.HTML('analyze.html')

"""
# Uploading January 2023 Outstanding Premium Data
Similar error was encountered when uploading January 2023 data ie UNICODEDECODEERROR: ‘UTF-8’. There were recomendations to use Chardet to fix it but we decided to standardise with the codes we had used for Dec 2022 which is applying the encoding = 'latin1'. 
"""

file1 = "BUJAN2023.csv"
BUJ = pd.read_csv(file1, encoding = "latin1", dtype={'2 MTHS': 'str', '3 MTHS': 'str','7 MTHS': 'str','8 MTHS': 'str','10 MTHS': 'str','SST ': 'str','0-14 DAYS': 'str','15-30 DAYS': 'str','1 MTH': 'str','4 MTHS': 'str', '5 MTHS': 'str', '6 MTHS': 'str','9 MTHS': 'str','11 MTHS': 'str','12 MTHS': 'str','CLIENT NO': 'str','CLIENT NO.2': 'str','AGNTNUM': 'str','>12 MTHS': 'str','TOTAL': 'str','CLT.1': 'str'})

"""Checking the number of records in January 2023  and the number of fields. """

BUJ.shape

"""List down fields in Jan 2023 BU Listing and replace the space with underscore"""

BUJ.columns = BUJ.columns.str.replace(' ', '_')
BUJ.info()

"""Codes to address the errors encountered in our attempt to convert the data type of Dec 2022 fields containing numerical values to 'float' (since this data type allows its value to be recognised as integer). Similar to earlier data, the codes have to be individually run for each field to avoid errors but this time, individual code for each line has to be performed."""

BUD['1_MTH'] = BUD['1_MTH'].str.replace(',','')
BUD['1_MTH'] = BUD['1_MTH'].astype(float, errors = 'raise')

BUD['2_MTHS'] = BUD['2_MTHS'].str.replace(',','')
BUD['2_MTHS'] = BUD['2_MTHS'].astype(float, errors = 'raise')

BUD['3_MTHS'] = BUD['3_MTHS'].str.replace(',','')
BUD['3_MTHS'] = BUD['3_MTHS'].astype(float, errors = 'raise')

BUD['4_MTHS'] = BUD['4_MTHS'].str.replace(',','')
BUD['4_MTHS'] = BUD['4_MTHS'].astype(float, errors = 'raise')

BUD['5_MTHS'] = BUD['5_MTHS'].str.replace(',','')
BUD['5_MTHS'] = BUD['5_MTHS'].astype(float, errors = 'raise')

BUD['6_MTHS'] = BUD['6_MTHS'].str.replace(',','')
BUD['6_MTHS'] = BUD['6_MTHS'].astype(float, errors = 'raise')

BUD['7_MTHS'] = BUD['7_MTHS'].str.replace(',','')
BUD['7_MTHS'] = BUD['7_MTHS'].astype(float, errors = 'raise')

BUD['8_MTHS'] = BUD['8_MTHS'].str.replace(',','')
BUD['8_MTHS'] = BUD['8_MTHS'].astype(float, errors = 'raise')

BUD['9_MTHS'] = BUD['9_MTHS'].str.replace(',','')
BUD['9_MTHS'] = BUD['9_MTHS'].astype(float, errors = 'raise')

BUD['10_MTHS'] = BUD['10_MTHS'].str.replace(',','')
BUD['10_MTHS'] = BUD['10_MTHS'].astype(float, errors = 'raise')

BUD['11_MTHS'] = BUD['11_MTHS'].str.replace(',','')
BUD['11_MTHS'] = BUD['11_MTHS'].astype(float, errors = 'raise')

BUD['12_MTHS'] = BUD['12_MTHS'].str.replace(',','')
BUD['12_MTHS'] = BUD['12_MTHS'].astype(float, errors = 'raise')

BUD['>12_MTHS'] = BUD['>12_MTHS'].str.replace(',','')
BUD['>12_MTHS'] = BUD['>12_MTHS'].astype(float, errors = 'raise')

"""Checking the data after conversion"""

BUD.info()

"""Codes to determine overdue premium for Dec 2022 data. 
1. Motor overdue =  records with outstanding aged more than 1 month. 
2.Non Motor overdue = records with outstanding aged more than 2 months. 
"""

BUD['total_motor'] = BUD.loc[(BUD['FundType'] == 'M') & (BUD['Trans_Type'] != 'Recn') & (BUD['Trans_Type'] != 'Rcpt'),['1_MTH','2_MTHS','3_MTHS','4_MTHS','5_MTHS','6_MTHS','7_MTHS','8_MTHS','9_MTHS','10_MTHS','11_MTHS','12_MTHS','>12_MTHS']].sum(axis=1)
BUD['total_nmotor'] =  BUD.loc[(BUD['FundType'] == 'NM') & (BUD['Trans_Type'] != 'Recn') & (BUD['Trans_Type'] != 'Rcpt'),['2_MTHS','3_MTHS','4_MTHS','5_MTHS','6_MTHS','7_MTHS','8_MTHS','9_MTHS','10_MTHS','11_MTHS','12_MTHS','>12_MTHS']].sum(axis=1)

"""Checking the total amount of motor and non-motor outstanding for Dec 2022 position."""

BUD['total_motor'].sum()

BUD['total_nmotor'].sum()

"""Determining records with overdue premium (Defaulted in payment within the credit term) and create as new column."""

BUD['DefaultM'] = BUD['total_motor']>0

BUD['DefaultNM'] = BUD['total_nmotor']>0

"""Codes to convert Jan 2023 data type from object to float."""

BUJ['1_MTH'] = BUJ['1_MTH'].str.replace(',','')
BUJ['1_MTH'] = BUJ['1_MTH'].astype(float, errors = 'raise')

BUJ['2_MTHS'] = BUJ['2_MTHS'].str.replace(',','')
BUJ['2_MTHS'] = BUJ['2_MTHS'].astype(float, errors = 'raise')

BUJ['3_MTHS'] = BUJ['3_MTHS'].str.replace(',','')
BUJ['3_MTHS'] = BUJ['3_MTHS'].astype(float, errors = 'raise')

BUJ['4_MTHS'] = BUJ['4_MTHS'].str.replace(',','')
BUJ['4_MTHS'] = BUJ['4_MTHS'].astype(float, errors = 'raise')

BUJ['5_MTHS'] = BUJ['5_MTHS'].str.replace(',','')
BUJ['5_MTHS'] = BUJ['5_MTHS'].astype(float, errors = 'raise')

BUJ['6_MTHS'] = BUJ['6_MTHS'].str.replace(',','')
BUJ['6_MTHS'] = BUJ['6_MTHS'].astype(float, errors = 'raise')

BUJ['7_MTHS'] = BUJ['7_MTHS'].str.replace(',','')
BUJ['7_MTHS'] = BUJ['7_MTHS'].astype(float, errors = 'raise')

BUJ['8_MTHS'] = BUJ['8_MTHS'].str.replace(',','')
BUJ['8_MTHS'] = BUJ['8_MTHS'].astype(float, errors = 'raise')

BUJ['9_MTHS'] = BUJ['9_MTHS'].str.replace(',','')
BUJ['9_MTHS'] = BUJ['9_MTHS'].astype(float, errors = 'raise')

BUJ['10_MTHS'] = BUJ['10_MTHS'].str.replace(',','')
BUJ['10_MTHS'] = BUJ['10_MTHS'].astype(float, errors = 'raise')

BUJ['11_MTHS'] = BUJ['11_MTHS'].str.replace(',','')
BUJ['11_MTHS'] = BUJ['11_MTHS'].astype(float, errors = 'raise')

BUJ['12_MTHS'] = BUJ['12_MTHS'].str.replace(',','')
BUJ['12_MTHS'] = BUJ['12_MTHS'].astype(float, errors = 'raise')

BUJ['>12_MTHS'] = BUJ['>12_MTHS'].str.replace(',','')
BUJ['>12_MTHS'] = BUJ['>12_MTHS'].astype(float, errors = 'raise')

"""Checking after conversion"""

BUJ.info()

"""Codes to determine overdue premium of motor and non-motor for January 2023 data"""

BUJ['total_motor'] = BUJ.loc[(BUJ['FundType'] == 'M') & (BUJ['Trans_Type'] != 'Recn') & (BUJ['Trans_Type'] != 'Rcpt'),['1_MTH','2_MTHS','3_MTHS','4_MTHS','5_MTHS','6_MTHS','7_MTHS','8_MTHS','9_MTHS','10_MTHS','11_MTHS','12_MTHS','>12_MTHS']].sum(axis=1)
BUJ['total_nmotor'] =  BUJ.loc[(BUJ['FundType'] == 'NM') & (BUJ['Trans_Type'] != 'Recn') & (BUJ['Trans_Type'] != 'Rcpt'),['2_MTHS','3_MTHS','4_MTHS','5_MTHS','6_MTHS','7_MTHS','8_MTHS','9_MTHS','10_MTHS','11_MTHS','12_MTHS','>12_MTHS']].sum(axis=1)

"""To check the overall total of overdue premium of motor and non-motor"""

BUJ['total_motor'].sum()

BUJ['total_nmotor'].sum()

"""## **DATA VISUALISATION**
Generate bar chart to compare overdue premium as at Dec 2022 vs Jan 2023 by product type and company.
"""

import numpy as np
import matplotlib.pyplot as plt

X = ['DEC', 'JAN']
Motor= [4596042.37, 2909565.28]
c = ['blue', 'orange']



X_axis = np.arange(len(X))
plt.bar(X_axis - 0.1, Motor, 0.4, color= c, label = 'Motor')
plt.ylim(0,5000000)
plt.xticks(X_axis, X)
plt.xlabel("Motor")
plt.ylabel("Total")
plt.title("General Business Outstanding Records by \n Product Type: Motor")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = ['DEC', 'JAN']
NMotor= [104135324.68999997,275207801.0700002]
c = ['blue', 'orange']


X_axis = np.arange(len(X))
plt.bar(X_axis + 0.2, NMotor, 0.4, color= c, label = 'Non-Motor')
plt.ylim(0,300000000)
plt.xticks(X_axis, X)
plt.xlabel("Motor")
plt.ylabel("Total")
plt.title("General Business Outstanding Records by \n Product Type: Non-Motor")
plt.legend()
plt.show()

BUD.groupby(["Company"])["total_motor"].sum()

BUD.groupby(["Company"])["total_nmotor"].sum()

BUJ.groupby(["Company"])["total_motor"].sum()

BUJ.groupby(["Company"])["total_nmotor"].sum()

import numpy as np
import matplotlib.pyplot as plt

X = ['Motor', 'Non-Motor']
Dec= [407734.95, 71019912.67]
Jan= [-122550.58, 1.846510e+08]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.1, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(-200000,200000000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGIB")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = ['Motor']
Dec= [407734.95]
Jan= [-122550.58]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.1, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(min(Jan),600000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGIB")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = [ 'Non-Motor']
Dec= [71019912.67]
Jan= [1.846510e+08]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.1, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(50000000,200000000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGIB")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = ['Motor', 'Non-Motor']
Dec= [4188307.42, 33115412.02]
Jan= [3032115.86, 9.055680e+07]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(-200000,100000000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGTB")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = ['Motor']
Dec= [4188307.42]
Jan= [3032115.86]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(0,5000000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGTB")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

X = ['Non-Motor']
Dec= [33115412.02]
Jan= [9.055680e+07]


X_axis = np.arange(len(X))

plt.bar(X_axis - 0.2, Dec, 0.4, label = 'December')
plt.bar(X_axis + 0.2, Jan, 0.4, label = 'January')
plt.ylim(0,100000000)
plt.xticks(X_axis, X)
plt.xlabel("FundType")
plt.ylabel("Total")
plt.title("EGTB")
plt.legend()
plt.show()

"""Using Sweetviz as one of the tools for data visualisation to derive the movement trend between the 2 months' data"""

import sweetviz as sv
compare_report = sv.compare([BUD, 'BU_Dec22'], [BUJ, 'BU_Jan23'])
compare_report.show_html('compare.html', open_browser=False)

"""Now let us witness the magic...TADA! """

IPython.display.HTML('compare.html')

"""# Quick Analysis Based On Sweetviz Graphs
EGTB Outstanding has increased by 1% if compared against Dec 22

EGIB Outstanding has reduced by 1% if compared against Dec 22

## **Predicting Possible Counterparties Delinquency Using Machine Learning**

---

We chose outstanding records for position as at 31 December 2022 as our base for predictive analysis.
We selected sales channel and region as the determining factors which we believe to have strong correlation with the overdue premium amongst counterparties.

Rather than using the whole entire fields, we decided to only select fields that are relevant to our prediction task.
"""

BUD.info()

"""We have identified 8 fields to be relevant where we renamed this new dataframe as BUD1 to represent the data set of Dec 2022."""

BUD.info()

BUD1= BUD[['SALES_CHANNEL','REGION','12_MTHS', '>12_MTHS','total_motor','total_nmotor','DefaultM', 'DefaultNM']].copy()
BUD1

BUD1.info()

rows, columns = BUD1.shape
print('Rows:', rows)
print('Columns:', columns)

"""We noticed quite a number of records with null or invalid value and thus, replacing them with zero value. This is necessary considering Predictive Model will encounter error with null value (treated as invalid value)."""

BUD1.isnull().sum()

"""Listing the 8 selected columns"""

BUD1.columns

"""To come up with a descriptive statistics summary of Dec 2022 (BUD1 after revision) dataframe



"""

BUD1.describe()

"""Checked 98% of the numerical values and then replace any non positive infinity values with zero value since infinity value will not be able to be recognised for predictive analysis too apart from null value."""

value = BUD1['total_motor'].quantile(0.98)
BUD1 = BUD1.replace(np.inf, value)

value = BUD1['total_nmotor'].quantile(0.98)
BUD1 = BUD1.replace(np.inf, value)

value = BUD1['12_MTHS'].quantile(0.98)
BUD1 = BUD1.replace(np.inf, value)

value = BUD1['>12_MTHS'].quantile(0.98)
BUD1 = BUD1.replace(np.inf, value)

"""Analysing BUD1 data for preparation in coming up with suitable predictive model"""

BUD1.hist( figsize = (22, 20) )
plt.show()

"""Determining the correlation within the selected fields based on Dec 2022 position."""

fig, ax = plt.subplots( figsize = (12,8) )
corr_matrix = BUD1.corr()
corr_heatmap = sns.heatmap( corr_matrix, cmap = "flare", annot=True, ax=ax, annot_kws={"size": 14})
plt.show()

def categorical_valcount_hist(feature):
    print(BUD1[feature].value_counts())
    fig, ax = plt.subplots( figsize = (6,6) )
    sns.countplot(x=feature, ax=ax, BUD1=BUD1)
    plt.show()

"""List possible category values in Sales Channel and Region."""

print( "Total categories in SALES CHANNEL:", len( BUD1["SALES_CHANNEL"].unique() ) )
print()
print( BUD1["SALES_CHANNEL"].value_counts() )

print( "Total categories in REGION:", len( BUD1["REGION"].unique() ) )
print()
print( BUD1["REGION"].value_counts() )

"""Using Seaborn Catplot to estimate the tendency of motor and non-motor overdue premium for each category of Sales Channel and Region.
(Ignoring the Width of the graph which can be adjusted)
"""

sns.set(font_scale=1)
sns.catplot(x='SALES_CHANNEL', y='total_motor', data=BUD1)

sns.catplot(x='SALES_CHANNEL', y='total_nmotor', data=BUD1)

"""Using Seaborn Box Plot to display the summary of the overdue premium by product type. """

sns.boxplot(x ="SALES_CHANNEL",y="total_motor" ,data = BUD1)

sns.boxplot(x ="SALES_CHANNEL",y="total_nmotor" ,data = BUD1)

"""Similar seaborn statistical tool for data visualisation being used based on REGION."""

sns.catplot(x='REGION', y='total_nmotor', data=BUD1)

sns.catplot(x='REGION', y='total_motor', data=BUD1)

sns.boxplot(x ="REGION",y="total_motor" ,data = BUD1)

sns.boxplot(x ="REGION",y="total_nmotor" ,data = BUD1)

"""Using subplot graph to visualise the total overdue premium by CHANNEL and REGION."""

import matplotlib.pyplot as plt
from matplotlib import gridspec

fig, ax = plt.subplots( figsize = (25,8) )
# create grid for different subplots
spec = gridspec.GridSpec(ncols=2, nrows=2,
                         width_ratios=[2, 1], wspace=0.5,
                         hspace=0.5, height_ratios=[1, 2])
 # initializing x,y axis value
x = np.arange(0, 10, 0.1)
y = np.cos(x)
 
# automatically adjust padding horizontally
# as well as vertically.
plt.tight_layout()
sns.countplot(x='SALES_CHANNEL', hue='REGION', ax=ax, data=BUD1)

fig, ax = plt.subplots( figsize = (10,8) )
sns.boxplot(x = "total_motor", y = "SALES_CHANNEL", hue='REGION', data = BUD1)

"""Converting a non-numerical categorical values (String) into numerical ie assign number to represent the values for the purpose of prediction (ML algorithms work better with numerical inputs)"""

pip install category-encoders

print( "Total categories in SALES CHANNEL:", len( BUD1["SALES_CHANNEL"].unique() ) )
print()
print(BUD1["SALES_CHANNEL"] .value_counts() )

"""Replacing Space with Underscore in the values of sales channel and region"""

print( "Total categories in SALES CHANNEL:", len( BUD1["SALES_CHANNEL"].unique() ) )
print()
print(BUD1["SALES_CHANNEL"] .value_counts() )

print( "Total categories in REGION:", len( BUD1["REGION"].unique() ) )
print()
print( BUD1["REGION"].value_counts() )

BUD1.replace('\s+', '_',regex=True,inplace=True)

"""Assigning the codes to Sales Channel and Region but still maintaining the original columns (using OneHot Encoder)"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import category_encoders as ce

labelencoder = LabelEncoder()
BUD1['CHANNEL_CODE'] = labelencoder.fit_transform(BUD1['SALES_CHANNEL'])

labelencoder = LabelEncoder()
BUD1['REGION_CODE'] = labelencoder.fit_transform(BUD1['REGION'])

labelencoder = LabelEncoder()
BUD1['DefaultM_Code'] = labelencoder.fit_transform(BUD1['DefaultM'])

labelencoder = LabelEncoder()
BUD1['DefaultNM_Code'] = labelencoder.fit_transform(BUD1['DefaultNM'])

BUD1.info()

import pandas as pd  
from sklearn import preprocessing   
blk = pd.DataFrame( BUD1 )  
print("Genuine Data Frame:\n")  
print( blk )

blk.fillna('', inplace=True)

blk['total_nmotor'] = blk['total_nmotor'].replace("", 0)

blk.head()

print(BUD1)

"""Replacing Null Values with Zero"""

BUD1.fillna('', inplace=True)

BUD1

BUD1['total_motor']=pd.to_numeric(BUD1['total_motor'],errors='coerce')
BUD1['total_nmotor']=pd.to_numeric(BUD1['total_nmotor'],errors='coerce')
BUD1['12_MTHS']=pd.to_numeric(BUD1['12_MTHS'],errors='coerce')
BUD1['>12_MTHS']=pd.to_numeric(BUD1['>12_MTHS'],errors='coerce')

BUD1.info()

BUD1.fillna('', inplace=True)

BUD1= BUD1[['SALES_CHANNEL','CHANNEL_CODE','REGION','12_MTHS', '>12_MTHS','total_motor','total_nmotor','DefaultM','DefaultM_Code','DefaultNM','DefaultNM_Code']].copy()

BUD1['total_motor'] = BUD1['total_motor'].replace("", 0)
BUD1['total_nmotor'] = BUD1['total_nmotor'].replace("", 0)
BUD1['12_MTHS'] = BUD1['12_MTHS'].replace("", 0)
BUD1['>12_MTHS'] = BUD1['12_MTHS'].replace("", 0)
BUD1.head()

BUD1.info()

"""# **Predictive Model Using Gaussian Naive Bayes **
Gaussian Naive Bayes (GNB) is a classification technique used in Machine Learning (ML) based on the probabilistic approach and Gaussian distribution. Gaussian Naive Bayes assumes that each parameter (also called features or predictors) has an independent capacity of predicting the output variable.

We find this statistical method to be the most suitable for our predictive analysis considering no dependency within SALES CHANNEL AND REGION with all categories of outstanding premium balances.

**Predictive analysis based on Sales Channel Code and Motor Overdue Premium**

Defining v and w values.

Note:The output may differ everytime we run the codes due to dataset is randomly selected.
"""

v= BUD1[['CHANNEL_CODE']]
w= BUD1[['DefaultM_Code']]

"""Using 20% as data set for predictive model"""

from sklearn.model_selection import train_test_split
v_train, v_test, w_train, w_test = train_test_split(v, w, test_size = 0.2, random_state= 7)

v_train.info()

w_train.info()

v_train

from sklearn.model_selection import train_test_split
w_train.value_counts()

v_train.value_counts()

from sklearn.datasets import load_digits
digits = load_digits()
v = digits.data
w = digits.target

v

w

"""Run predictive model based on Motor Overdue"""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split

# split the data into training and validation sets
v_train, v_test, w_train, w_test = train_test_split(digits.data, digits.target)

# train the model
clf = GaussianNB()
clf.fit(v_train, w_train)


# use the model to predict the labels of the test data
predicted = clf.predict(v_test)
expected = w_test
print(predicted)

v_test

"""Total no of records which do not match"""

matches = (predicted == expected)
print(matches.sum())

"""List the predictive classification model metrics"""

from sklearn import metrics
print(metrics.classification_report(expected, predicted))

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

BUD1.describe()

"""**Predictive Model using Non-Motor Overdue Premium vs Sales Channel as the basis.**

Defining x and y values. 

Note: The output may differ everytime we run the codes due to dataset is randomly selected.
"""

x= BUD1[['CHANNEL_CODE']]
y= BUD1[['DefaultNM_Code']]

"""Using 20% as data set for predictive model"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state= 7)

x_train.info()

x_train

from sklearn.model_selection import train_test_split
y_train.value_counts()

x_train.value_counts()

print( "Total categories in SALES_CHANNEL:", len( BUD1["SALES_CHANNEL"].unique() ) )
print()
print( BUD1["SALES_CHANNEL"].value_counts() )

"""To simplify the predicted result, we are limiting the Sales Channel output to 10 out of the overall 21 Sales Channels. """

from sklearn.datasets import load_digits
digits = load_digits()
x = digits.data
y = digits.target

x

y

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split

# split the data into training and validation sets
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target)

# train the model
clf = GaussianNB()
clf.fit(x_train, y_train)


# use the model to predict the labels of the test data
predicted = clf.predict(x_test)
expected = y_test
print(predicted)

x_test

"""Total no of records which did not match."""

matches = (predicted == expected)
print(matches.sum())

"""List down the predictive classification report metrics"""

from sklearn import metrics
print(metrics.classification_report(expected, predicted))

"""# **How to read metric results in Classification Report**
**Recall**: the ability of a classification model to identify all data points in a relevant class

**Precision**: the ability of a classification model to return only the data points in a class

**F1 score**: a single metric that combines recall and precision using the harmonic mean

**Support** is the number of occurrences of each class in y_true 

*Another definitions*:

Precision quantifies the number of positive class predictions that actually belong to the positive class.

Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.

F1 Score-Measure provides a single score that balances both the concerns of precision and recall in one number.

**Based on the F1 score using combination of sales channel and Default Code, the overall best model occurs at Combination 0 (but with lesser number of occurrence. The overall level of accuracy stood at 84%**

Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial
Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.
In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on.

# **What is Confusion Matrix?**

Confusion matrix: shows the actual and predicted labels from a classification problem

The confusion matrix provides more insight into not only the performance of a predictive model, but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made.

There are four categories in Confusion Matrix:

True Positive (TN) - This is correctly classified as the class if interest / target.

True Negative (TN) - This is correctly classified as not a class of interest / target.

False Positive (FP) - This is wrongly classified as the class of interest / target.

False Negative (FN) - This is wrongly classified as not a class of interest / target.

**Confusion Matrix for Motor Overdue Premium**
"""

print(metrics.confusion_matrix(expected, predicted))

"""**Confusion Matrix Non Motor Overdue Premium**"""

print(metrics.confusion_matrix(expected, predicted))

"""## **Future Predictive Model Plan**

We plan to repeat the above predictive analysis using the following combinations:

a. Total Default Non-Motor vs Sales Channel

b  Total 12 Months vs Sales Channel

c. Total > 12 Months vs Sales Channel

d. Total Default Motor vs Region

e. Total Default Non-Motor vs Region

f. Total 12 Months vs Region

g  Total > 12 Months vs Region

*Codes to combine Sales Channel and Region*

Go even further by combining Sales Channel and Region to be the basis against the 4 categories of overdue premium. The following are the codes to combine the 2 fields. 
Predictive model codes would be the same as the above where only need to change the field for 'x'.
"""

# Using apply method to combine values
BUD1['COMBINED_CHANNEL_REGION_1'] = BUD1[BUD1.columns[0:2]].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)

# Display modified DataFrame
print("COMBINED_CHANNEL_REGION_1:\n",BUD1)

print( "Total categories in a combination of SALES CHANNEL AND REGION:", len( BUD1["COMBINED_CHANNEL_REGION_1"].unique() ) )
print()
print(BUD1["COMBINED_CHANNEL_REGION_1"] .value_counts() )

BUD1.fillna('', inplace=True)